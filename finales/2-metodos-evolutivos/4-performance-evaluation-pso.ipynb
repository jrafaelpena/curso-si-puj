{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cfe2a19",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5650c1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import seaborn as sns\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.nn import Sequential, Linear, ReLU, Dropout, BatchNorm1d\n",
    "from train import _compute_metrics, train_epoch, eval_epoch, train_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from train import train_model\n",
    "import pickle\n",
    "import optunahub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e4d07bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = Path(Path.cwd()).parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee5d683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 7777"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aacacc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dummy:\n",
    "    \"\"\"Universal dummy class to safely replace missing modules/classes during unpickling.\"\"\"\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super().__new__(cls)\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "    def __getattr__(self, name):\n",
    "        return self\n",
    "    def __setattr__(self, name, value):\n",
    "        pass\n",
    "    def __getitem__(self, key):\n",
    "        return self\n",
    "    def __iter__(self):\n",
    "        return iter([])\n",
    "    def __repr__(self):\n",
    "        return \"<Dummy>\"\n",
    "\n",
    "class SafeUnpickler(pickle.Unpickler):\n",
    "    def find_class(self, module, name):\n",
    "        if module.startswith(\"optunahub_registry\"):\n",
    "            return Dummy\n",
    "        return super().find_class(module, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e96d6b",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f83d5438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>smoking</th>\n",
       "      <th>finger_discoloration</th>\n",
       "      <th>mental_stress</th>\n",
       "      <th>exposure_to_pollution</th>\n",
       "      <th>long_term_illness</th>\n",
       "      <th>energy_level</th>\n",
       "      <th>immune_weakness</th>\n",
       "      <th>breathing_issue</th>\n",
       "      <th>alcohol_consumption</th>\n",
       "      <th>throat_discomfort</th>\n",
       "      <th>oxygen_saturation</th>\n",
       "      <th>chest_tightness</th>\n",
       "      <th>family_history</th>\n",
       "      <th>smoking_family_history</th>\n",
       "      <th>stress_immune</th>\n",
       "      <th>pulmonary_disease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.831178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>95.977287</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>47.694835</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.184483</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59.577435</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94.974939</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59.785767</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95.187900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59.733941</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>93.503008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.684285</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>94.057151</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>52.647022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>96.773598</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53.306451</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>95.019018</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>64.272789</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98.539379</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.319319</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>96.055097</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  smoking  finger_discoloration  mental_stress  \\\n",
       "0   68       1        1                     1              1   \n",
       "1   81       1        1                     0              0   \n",
       "2   58       1        1                     0              0   \n",
       "3   44       0        1                     0              1   \n",
       "4   72       0        1                     1              1   \n",
       "5   37       1        1                     1              1   \n",
       "6   50       0        1                     1              1   \n",
       "7   68       0        1                     1              1   \n",
       "8   48       0        1                     1              0   \n",
       "9   52       0        0                     0              1   \n",
       "\n",
       "   exposure_to_pollution  long_term_illness  energy_level  immune_weakness  \\\n",
       "0                      1                  0     57.831178                0   \n",
       "1                      1                  1     47.694835                1   \n",
       "2                      0                  0     59.577435                0   \n",
       "3                      1                  0     59.785767                0   \n",
       "4                      1                  1     59.733941                0   \n",
       "5                      1                  1     57.684285                0   \n",
       "6                      0                  1     52.647022                1   \n",
       "7                      0                  1     53.306451                0   \n",
       "8                      1                  1     64.272789                1   \n",
       "9                      1                  1     58.319319                0   \n",
       "\n",
       "   breathing_issue  alcohol_consumption  throat_discomfort  oxygen_saturation  \\\n",
       "0                0                    1                  1          95.977287   \n",
       "1                1                    0                  1          97.184483   \n",
       "2                1                    1                  0          94.974939   \n",
       "3                1                    0                  1          95.187900   \n",
       "4                1                    0                  1          93.503008   \n",
       "5                1                    1                  1          94.057151   \n",
       "6                1                    1                  0          96.773598   \n",
       "7                0                    0                  1          95.019018   \n",
       "8                1                    0                  1          98.539379   \n",
       "9                1                    0                  1          96.055097   \n",
       "\n",
       "   chest_tightness  family_history  smoking_family_history  stress_immune  \\\n",
       "0                1               0                       0              0   \n",
       "1                0               0                       0              0   \n",
       "2                0               0                       0              0   \n",
       "3                0               0                       0              0   \n",
       "4                0               0                       0              0   \n",
       "5                1               0                       0              0   \n",
       "6                0               0                       0              1   \n",
       "7                0               0                       0              0   \n",
       "8                1               0                       0              0   \n",
       "9                0               0                       0              0   \n",
       "\n",
       "  pulmonary_disease  \n",
       "0                NO  \n",
       "1               YES  \n",
       "2                NO  \n",
       "3               YES  \n",
       "4               YES  \n",
       "5               YES  \n",
       "6                NO  \n",
       "7                NO  \n",
       "8               YES  \n",
       "9                NO  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_detection_path = project_path / \"data/inputs/Lung Cancer Dataset.csv\"\n",
    "df_detection = pd.read_csv(cancer_detection_path)\n",
    "\n",
    "df_detection.columns = [x for x in df_detection.columns.str.lower().str.replace(\" \", \"_\")]\n",
    "\n",
    "df_detection.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e9e5be7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 18)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_detection.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce91874",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1bd042a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert label column to numerical values\n",
    "label_map_dict = {\n",
    "    'NO': 0,\n",
    "    'YES': 1\n",
    "}\n",
    "\n",
    "df_detection['pulmonary_disease'] = df_detection['pulmonary_disease'].map(label_map_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "499a2506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert binary columns to categorical\n",
    "binary_columns = [\n",
    "    'gender',\n",
    "    'smoking',\n",
    "    'finger_discoloration',\n",
    "    'mental_stress',\n",
    "    'exposure_to_pollution',\n",
    "    'long_term_illness',\n",
    "    'immune_weakness',\n",
    "    'breathing_issue',\n",
    "    'alcohol_consumption',\n",
    "    'throat_discomfort',\n",
    "    'chest_tightness',\n",
    "    'family_history',\n",
    "    'smoking_family_history',\n",
    "    'stress_immune',\n",
    "    'pulmonary_disease'\n",
    "]\n",
    "\n",
    "df_detection[binary_columns] = df_detection[binary_columns].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8672bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_detection.drop(columns=['pulmonary_disease']).values\n",
    "y = df_detection['pulmonary_disease'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=SEED, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.15, random_state=SEED, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "83fa335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shape: (3612, 17)\n",
      "Validation set shape: (638, 17)\n",
      "Test set shape: (750, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train set shape:\", X_train.shape)\n",
    "print(\"Validation set shape:\", X_val.shape)\n",
    "print(\"Test set shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2df9f2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([57.37015504,  0.49612403,  0.66196013,  0.60354374,  0.54512735,\n",
       "         0.51522702,  0.43992248, 54.99256295,  0.39451827,  0.79983389,\n",
       "         0.35022148,  0.69988926, 94.9913959 ,  0.6013289 ,  0.303433  ,\n",
       "         0.20265781,  0.21179402]),\n",
       " array([15.83079571,  0.49998498,  0.47304219,  0.48916121,  0.49795936,\n",
       "         0.49976808,  0.49637757,  7.84740972,  0.48874697,  0.40012453,\n",
       "         0.4770392 ,  0.45830589,  1.49321387,  0.48962481,  0.4597406 ,\n",
       "         0.40197963,  0.40857963]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.mean(axis=0), X_train.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "656fbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"ga_results.pkl\", \"rb\") as f:\n",
    "    ga_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e26891a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1]\n",
      "[ True False  True  True False  True False  True  True  True False  True\n",
      " False False  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "best_feature_set = ga_results['best_individual']\n",
    "best_feature_mask = np.array(best_feature_set, dtype=bool)\n",
    "print(best_feature_set)\n",
    "print(best_feature_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9d6a6724",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:,best_feature_mask]\n",
    "X_val = X_val[:,best_feature_mask]\n",
    "X_test = X_test[:,best_feature_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d090f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc393390",
   "metadata": {},
   "source": [
    "### PSO performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "56a26437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded successfully: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"pso_results_2.pkl\", \"rb\") as f:\n",
    "    pso_results = SafeUnpickler(f).load()\n",
    "\n",
    "print(\"Loaded successfully:\", type(pso_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32da61ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'use_batch_norm': True,\n",
       " 'learning_rate': 0.004355387279653677,\n",
       " 'weight_decay': 1e-06,\n",
       " 'batch_size': 96,\n",
       " 'hidden_size_0': 32,\n",
       " 'hidden_size_1': 16,\n",
       " 'hidden_size_2': 16,\n",
       " 'dropout_rate_0': 0.6,\n",
       " 'dropout_rate_1': 0.5086790371135944,\n",
       " 'dropout_rate_2': 0.1293341508821494}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = pso_results['best_params']\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ad573d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b1285ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture (with BatchNorm):\n",
      "============================================================\n",
      "Layer 0: Linear(in_features=11, out_features=32, bias=True)\n",
      "Layer 1: ReLU()\n",
      "Layer 2: BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Layer 3: Dropout(p=0.6, inplace=False)\n",
      "Layer 4: Linear(in_features=32, out_features=16, bias=True)\n",
      "Layer 5: ReLU()\n",
      "Layer 6: BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Layer 7: Dropout(p=0.5086790371135944, inplace=False)\n",
      "Layer 8: Linear(in_features=16, out_features=16, bias=True)\n",
      "Layer 9: ReLU()\n",
      "Layer 10: BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "Layer 11: Dropout(p=0.1293341508821494, inplace=False)\n",
      "Layer 12: Linear(in_features=16, out_features=2, bias=True)\n",
      "============================================================\n",
      "\n",
      "Total parameters: 1,346\n",
      "Trainable parameters: 1,346\n"
     ]
    }
   ],
   "source": [
    "# Your best parameters from PSO\n",
    "best_params = pso_results['best_params']\n",
    "use_batch_norm = best_params['use_batch_norm']\n",
    "learning_rate = best_params['learning_rate']\n",
    "weight_decay = best_params['weight_decay']\n",
    "batch_size = best_params['batch_size']\n",
    "hidden_size_0 = best_params['hidden_size_0']\n",
    "hidden_size_1 = best_params['hidden_size_1']\n",
    "hidden_size_2 = best_params['hidden_size_2']\n",
    "dropout_rate_0 = best_params['dropout_rate_0']\n",
    "dropout_rate_1 = best_params['dropout_rate_1']\n",
    "dropout_rate_2 = best_params['dropout_rate_2']\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32), \n",
    "    torch.tensor(y_train, dtype=torch.long)\n",
    ")\n",
    "val_dataset = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32), \n",
    "    torch.tensor(y_val, dtype=torch.long)\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(X_test, dtype=torch.float32), \n",
    "    torch.tensor(y_test, dtype=torch.long)\n",
    ")\n",
    "\n",
    "# Create model WITH batch normalization\n",
    "if use_batch_norm:\n",
    "    mlp = Sequential(\n",
    "        # First layer\n",
    "        Linear(11, hidden_size_0),\n",
    "        ReLU(),\n",
    "        BatchNorm1d(hidden_size_0),  # BatchNorm AFTER activation\n",
    "        Dropout(dropout_rate_0),\n",
    "        \n",
    "        # Second layer\n",
    "        Linear(hidden_size_0, hidden_size_1),\n",
    "        ReLU(),\n",
    "        BatchNorm1d(hidden_size_1),  # BatchNorm AFTER activation\n",
    "        Dropout(dropout_rate_1),\n",
    "        \n",
    "        # Third layer\n",
    "        Linear(hidden_size_1, hidden_size_2),\n",
    "        ReLU(),\n",
    "        BatchNorm1d(hidden_size_2),  # BatchNorm AFTER activation\n",
    "        Dropout(dropout_rate_2),\n",
    "        \n",
    "        # Output layer (no BatchNorm on output)\n",
    "        Linear(hidden_size_2, 2)\n",
    "    )\n",
    "else:\n",
    "    # Without batch norm (your original code)\n",
    "    mlp = Sequential(\n",
    "        Linear(11, hidden_size_0),\n",
    "        ReLU(),\n",
    "        Dropout(dropout_rate_0),\n",
    "        Linear(hidden_size_0, hidden_size_1),\n",
    "        ReLU(),\n",
    "        Dropout(dropout_rate_1),\n",
    "        Linear(hidden_size_1, hidden_size_2),\n",
    "        ReLU(),\n",
    "        Dropout(dropout_rate_2),\n",
    "        Linear(hidden_size_2, 2)\n",
    "    )\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Create data loaders with optimized batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Model Architecture (with BatchNorm):\")\n",
    "print(\"=\" * 60)\n",
    "for i, layer in enumerate(mlp):\n",
    "    print(f\"Layer {i}: {layer}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in mlp.parameters())\n",
    "trainable_params = sum(p.numel() for p in mlp.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "823415ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 300 epochs with patience=75\n",
      "====================================================================================================\n",
      "Epoch   1/300 | Train: Loss 0.6481 Acc 61.85% F1 0.5540 AUROC 0.6648 | Val: Loss 0.4666 Acc 85.27% F1 0.8066 AUROC 0.9213\n",
      "Epoch  10/300 | Train: Loss 0.3322 Acc 88.93% F1 0.8628 AUROC 0.9183 | Val: Loss 0.2453 Acc 92.48% F1 0.9055 AUROC 0.9467\n",
      "Epoch  20/300 | Train: Loss 0.3162 Acc 89.65% F1 0.8732 AUROC 0.9203 | Val: Loss 0.2502 Acc 92.01% F1 0.9014 AUROC 0.9503\n",
      "Epoch  30/300 | Train: Loss 0.3126 Acc 89.95% F1 0.8756 AUROC 0.9198 | Val: Loss 0.2361 Acc 93.42% F1 0.9180 AUROC 0.9498\n",
      "Epoch  40/300 | Train: Loss 0.3097 Acc 90.25% F1 0.8790 AUROC 0.9185 | Val: Loss 0.2426 Acc 92.63% F1 0.9084 AUROC 0.9453\n",
      "Epoch  50/300 | Train: Loss 0.3108 Acc 90.23% F1 0.8802 AUROC 0.9190 | Val: Loss 0.2300 Acc 93.26% F1 0.9165 AUROC 0.9499\n",
      "Epoch  60/300 | Train: Loss 0.3078 Acc 90.31% F1 0.8808 AUROC 0.9180 | Val: Loss 0.2411 Acc 93.57% F1 0.9207 AUROC 0.9488\n",
      "Epoch  70/300 | Train: Loss 0.3095 Acc 90.39% F1 0.8814 AUROC 0.9134 | Val: Loss 0.2363 Acc 93.26% F1 0.9171 AUROC 0.9484\n",
      "Epoch  80/300 | Train: Loss 0.3019 Acc 90.34% F1 0.8807 AUROC 0.9229 | Val: Loss 0.2372 Acc 92.79% F1 0.9098 AUROC 0.9485\n",
      "Epoch  90/300 | Train: Loss 0.3019 Acc 90.53% F1 0.8828 AUROC 0.9241 | Val: Loss 0.2294 Acc 93.10% F1 0.9151 AUROC 0.9485\n",
      "Epoch 100/300 | Train: Loss 0.3020 Acc 90.53% F1 0.8838 AUROC 0.9211 | Val: Loss 0.2343 Acc 93.10% F1 0.9151 AUROC 0.9492\n",
      "Epoch 110/300 | Train: Loss 0.3026 Acc 90.50% F1 0.8835 AUROC 0.9199 | Val: Loss 0.2278 Acc 93.57% F1 0.9210 AUROC 0.9491\n",
      "Epoch 120/300 | Train: Loss 0.3077 Acc 89.92% F1 0.8755 AUROC 0.9189 | Val: Loss 0.2333 Acc 92.79% F1 0.9109 AUROC 0.9483\n",
      "Epoch 130/300 | Train: Loss 0.2958 Acc 90.39% F1 0.8811 AUROC 0.9263 | Val: Loss 0.2268 Acc 93.10% F1 0.9154 AUROC 0.9482\n",
      "Epoch 140/300 | Train: Loss 0.2987 Acc 90.84% F1 0.8874 AUROC 0.9238 | Val: Loss 0.2292 Acc 93.26% F1 0.9178 AUROC 0.9492\n",
      "Epoch 150/300 | Train: Loss 0.2942 Acc 90.70% F1 0.8857 AUROC 0.9270 | Val: Loss 0.2214 Acc 93.57% F1 0.9210 AUROC 0.9501\n",
      "Epoch 160/300 | Train: Loss 0.2991 Acc 90.39% F1 0.8816 AUROC 0.9254 | Val: Loss 0.2308 Acc 93.26% F1 0.9165 AUROC 0.9500\n",
      "Epoch 170/300 | Train: Loss 0.3008 Acc 90.20% F1 0.8798 AUROC 0.9252 | Val: Loss 0.2372 Acc 93.42% F1 0.9189 AUROC 0.9503\n",
      "Epoch 180/300 | Train: Loss 0.3001 Acc 90.28% F1 0.8807 AUROC 0.9249 | Val: Loss 0.2286 Acc 93.26% F1 0.9165 AUROC 0.9493\n",
      "Epoch 190/300 | Train: Loss 0.3060 Acc 90.37% F1 0.8817 AUROC 0.9212 | Val: Loss 0.2272 Acc 93.42% F1 0.9189 AUROC 0.9503\n",
      "Epoch 200/300 | Train: Loss 0.3016 Acc 90.39% F1 0.8825 AUROC 0.9240 | Val: Loss 0.2233 Acc 93.26% F1 0.9168 AUROC 0.9495\n",
      "Epoch 210/300 | Train: Loss 0.2968 Acc 90.39% F1 0.8815 AUROC 0.9263 | Val: Loss 0.2233 Acc 94.04% F1 0.9264 AUROC 0.9496\n",
      "Epoch 220/300 | Train: Loss 0.3016 Acc 90.67% F1 0.8849 AUROC 0.9210 | Val: Loss 0.2319 Acc 93.57% F1 0.9201 AUROC 0.9515\n",
      "Epoch 230/300 | Train: Loss 0.2921 Acc 91.06% F1 0.8893 AUROC 0.9272 | Val: Loss 0.2337 Acc 93.26% F1 0.9171 AUROC 0.9518\n",
      "Epoch 240/300 | Train: Loss 0.3022 Acc 90.67% F1 0.8850 AUROC 0.9202 | Val: Loss 0.2265 Acc 93.73% F1 0.9222 AUROC 0.9518\n",
      "Epoch 250/300 | Train: Loss 0.2994 Acc 90.50% F1 0.8836 AUROC 0.9240 | Val: Loss 0.2304 Acc 93.57% F1 0.9194 AUROC 0.9495\n",
      "Epoch 260/300 | Train: Loss 0.2922 Acc 90.73% F1 0.8852 AUROC 0.9272 | Val: Loss 0.2226 Acc 93.57% F1 0.9210 AUROC 0.9522\n",
      "Epoch 270/300 | Train: Loss 0.3056 Acc 90.17% F1 0.8790 AUROC 0.9195 | Val: Loss 0.2227 Acc 93.89% F1 0.9246 AUROC 0.9526\n",
      "Epoch 280/300 | Train: Loss 0.2930 Acc 90.78% F1 0.8866 AUROC 0.9232 | Val: Loss 0.2298 Acc 94.04% F1 0.9266 AUROC 0.9523\n",
      "Epoch 290/300 | Train: Loss 0.3014 Acc 90.42% F1 0.8825 AUROC 0.9207 | Val: Loss 0.2282 Acc 93.26% F1 0.9168 AUROC 0.9520\n",
      "Epoch 300/300 | Train: Loss 0.2945 Acc 90.48% F1 0.8828 AUROC 0.9227 | Val: Loss 0.2249 Acc 93.73% F1 0.9222 AUROC 0.9524\n",
      "====================================================================================================\n",
      "Training completed. Best model from epoch 245\n",
      "Best validation loss: 0.2184\n"
     ]
    }
   ],
   "source": [
    "results = train_model(mlp, train_loader, val_loader, criterion, optimizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fc2518a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': {'train_loss': [0.6480894684791565,\n",
       "   0.5067323629444224,\n",
       "   0.41958325715160055,\n",
       "   0.38745966613094673,\n",
       "   0.36369902044039626,\n",
       "   0.3530406354867739,\n",
       "   0.34543462310518536,\n",
       "   0.3390187689434254,\n",
       "   0.3317756957786028,\n",
       "   0.3322473281740746,\n",
       "   0.3322278246333037,\n",
       "   0.32027956675057395,\n",
       "   0.32053898111530316,\n",
       "   0.31825077226787707,\n",
       "   0.32481922758774107,\n",
       "   0.3231137124604957,\n",
       "   0.3218630981009664,\n",
       "   0.3133955521639003,\n",
       "   0.3152867945721775,\n",
       "   0.3161604312369198,\n",
       "   0.3244854280124867,\n",
       "   0.30965123669649675,\n",
       "   0.31935147302491323,\n",
       "   0.30140225732841364,\n",
       "   0.3229357551696689,\n",
       "   0.31039020617142865,\n",
       "   0.31589120071987775,\n",
       "   0.3121609287800583,\n",
       "   0.3171883160093694,\n",
       "   0.31256204338564825,\n",
       "   0.30977453251811754,\n",
       "   0.31596082845003504,\n",
       "   0.3009523396872207,\n",
       "   0.3143972111896819,\n",
       "   0.31043925752671453,\n",
       "   0.30834243137179024,\n",
       "   0.31369625076503055,\n",
       "   0.31382785846426636,\n",
       "   0.3148805736703334,\n",
       "   0.30968328033174786,\n",
       "   0.30434166622716324,\n",
       "   0.306419899198304,\n",
       "   0.3081049905266873,\n",
       "   0.3056382001238408,\n",
       "   0.3084616413544183,\n",
       "   0.3095436851645625,\n",
       "   0.30320971946779673,\n",
       "   0.307752856087447,\n",
       "   0.3056414857061202,\n",
       "   0.31075648176313636,\n",
       "   0.3066383504194279,\n",
       "   0.3127686928871066,\n",
       "   0.3032139364469091,\n",
       "   0.3054904258528421,\n",
       "   0.3037556701918377,\n",
       "   0.30517940713321645,\n",
       "   0.31564569740596404,\n",
       "   0.3034609014112688,\n",
       "   0.3158735699233819,\n",
       "   0.3077538380789202,\n",
       "   0.30021893997921106,\n",
       "   0.2974819020179419,\n",
       "   0.3081976421845712,\n",
       "   0.3039847012374092,\n",
       "   0.3084783003575778,\n",
       "   0.3043879686399948,\n",
       "   0.30655784652478674,\n",
       "   0.307370788334216,\n",
       "   0.30057597006872244,\n",
       "   0.3095332980155945,\n",
       "   0.3051148717882625,\n",
       "   0.3051068109036284,\n",
       "   0.3082880361927704,\n",
       "   0.3068008106016241,\n",
       "   0.30311921486426824,\n",
       "   0.30503099661728866,\n",
       "   0.30644794843125583,\n",
       "   0.3035879920289366,\n",
       "   0.30845385185903884,\n",
       "   0.30187276223569215,\n",
       "   0.30886261704752216,\n",
       "   0.3074201267225006,\n",
       "   0.3052951182637896,\n",
       "   0.30208836183991544,\n",
       "   0.3025738091959906,\n",
       "   0.3091411525228887,\n",
       "   0.30336818792099174,\n",
       "   0.30202568458560297,\n",
       "   0.29953980777548794,\n",
       "   0.3019443509784648,\n",
       "   0.311622864978258,\n",
       "   0.29966718403999987,\n",
       "   0.30618493372815786,\n",
       "   0.30402029342825626,\n",
       "   0.3023074945045072,\n",
       "   0.30041632338417723,\n",
       "   0.3054690287754781,\n",
       "   0.297964360428807,\n",
       "   0.30269921043782533,\n",
       "   0.30204189699947637,\n",
       "   0.3033025874152928,\n",
       "   0.3040481603818874,\n",
       "   0.3056502674901208,\n",
       "   0.3080427442674225,\n",
       "   0.30378494919138493,\n",
       "   0.30540194587652075,\n",
       "   0.30559434075886227,\n",
       "   0.2973543657020873,\n",
       "   0.3003788748353819,\n",
       "   0.30260502519797644,\n",
       "   0.3068285288406765,\n",
       "   0.30855115426339186,\n",
       "   0.3084797127500325,\n",
       "   0.2978051352342498,\n",
       "   0.3076389703243674,\n",
       "   0.3085133299677079,\n",
       "   0.29933998300585635,\n",
       "   0.30320536803169507,\n",
       "   0.30249422323268116,\n",
       "   0.3077291601718066,\n",
       "   0.301941252923091,\n",
       "   0.3016665443233477,\n",
       "   0.29908008957622056,\n",
       "   0.3032151837384582,\n",
       "   0.30330577096669775,\n",
       "   0.30390390704240516,\n",
       "   0.30993840593436234,\n",
       "   0.30541450577320844,\n",
       "   0.30069177550730913,\n",
       "   0.2957799113618575,\n",
       "   0.3080820492731772,\n",
       "   0.30509872328601406,\n",
       "   0.3079996930800403,\n",
       "   0.29880201182888194,\n",
       "   0.308864506277135,\n",
       "   0.3007625695874921,\n",
       "   0.3015348738016084,\n",
       "   0.30649638076953317,\n",
       "   0.30051554090952953,\n",
       "   0.29872033297025485,\n",
       "   0.30922856748698163,\n",
       "   0.30509096775142064,\n",
       "   0.30229573778535834,\n",
       "   0.30069958391379675,\n",
       "   0.30412782614809336,\n",
       "   0.2921481372114036,\n",
       "   0.30026392579276695,\n",
       "   0.2973880096032374,\n",
       "   0.310087418140367,\n",
       "   0.2942137418197239,\n",
       "   0.29836583701875125,\n",
       "   0.29581923357078005,\n",
       "   0.3042336732050113,\n",
       "   0.2994101149497238,\n",
       "   0.30328954226550864,\n",
       "   0.31060933195475326,\n",
       "   0.2975075806296149,\n",
       "   0.29528904495841246,\n",
       "   0.2994575948018172,\n",
       "   0.2991258570324147,\n",
       "   0.30458760043711364,\n",
       "   0.3003522004201959,\n",
       "   0.2957030608012431,\n",
       "   0.3060045113595221,\n",
       "   0.2966904157023889,\n",
       "   0.3013041717467514,\n",
       "   0.2937976248636594,\n",
       "   0.2949937144882259,\n",
       "   0.3042241930961609,\n",
       "   0.3008443616157354,\n",
       "   0.3113841918417782,\n",
       "   0.29978700699996313,\n",
       "   0.29786754134683513,\n",
       "   0.29871477023311627,\n",
       "   0.29610734811652933,\n",
       "   0.29791902475974885,\n",
       "   0.3002740068491115,\n",
       "   0.29797715602127023,\n",
       "   0.30196975344835325,\n",
       "   0.30012488295865614,\n",
       "   0.2946497865293509,\n",
       "   0.3031867280156905,\n",
       "   0.3016302684117789,\n",
       "   0.29324049836773414,\n",
       "   0.30124671029490097,\n",
       "   0.30051146492213504,\n",
       "   0.3031492300603873,\n",
       "   0.30348514985800584,\n",
       "   0.3007369857293823,\n",
       "   0.3060471380865851,\n",
       "   0.2926915511736442,\n",
       "   0.3056419017505012,\n",
       "   0.3038549385593579,\n",
       "   0.30129245844394265,\n",
       "   0.29869762706598174,\n",
       "   0.2986219456524548,\n",
       "   0.3057417364611578,\n",
       "   0.30552147294199744,\n",
       "   0.2988116892667308,\n",
       "   0.301618281095923,\n",
       "   0.30347529251710126,\n",
       "   0.29705023795267277,\n",
       "   0.3026238896720037,\n",
       "   0.297721393728177,\n",
       "   0.29861019937897043,\n",
       "   0.2958447984187706,\n",
       "   0.30379862122955514,\n",
       "   0.3015166546815258,\n",
       "   0.29511118667862346,\n",
       "   0.2967740795343025,\n",
       "   0.2957928072772549,\n",
       "   0.30108124938518105,\n",
       "   0.3011073080507228,\n",
       "   0.3010837846618158,\n",
       "   0.294734626711405,\n",
       "   0.2975031519649037,\n",
       "   0.29769438450716657,\n",
       "   0.29885473193916373,\n",
       "   0.29927653618825234,\n",
       "   0.30160724973163733,\n",
       "   0.29586100890192873,\n",
       "   0.3009765172915601,\n",
       "   0.2937031804129135,\n",
       "   0.29352689155708517,\n",
       "   0.29519420258230544,\n",
       "   0.29936027749630306,\n",
       "   0.3013121129765463,\n",
       "   0.3023150119670602,\n",
       "   0.30001079729229113,\n",
       "   0.2920637838666225,\n",
       "   0.2995205198807574,\n",
       "   0.30146067141496463,\n",
       "   0.30337671895953905,\n",
       "   0.3014012608813289,\n",
       "   0.29532785362183456,\n",
       "   0.3009273006868521,\n",
       "   0.2974409794688621,\n",
       "   0.30439324325501327,\n",
       "   0.29797615209885214,\n",
       "   0.3022336058838423,\n",
       "   0.2969461978471952,\n",
       "   0.2990376122568137,\n",
       "   0.2999679918503048,\n",
       "   0.2970271233308355,\n",
       "   0.2994243035284784,\n",
       "   0.3030585129791716,\n",
       "   0.29061028678155815,\n",
       "   0.30277291663065303,\n",
       "   0.3003936069352286,\n",
       "   0.29935825059184207,\n",
       "   0.2946028468814799,\n",
       "   0.2962758685663293,\n",
       "   0.2971230932941468,\n",
       "   0.2978311777114868,\n",
       "   0.30587030160070655,\n",
       "   0.29667107350010413,\n",
       "   0.29301781501880914,\n",
       "   0.3026533498122447,\n",
       "   0.3002070433079602,\n",
       "   0.29218870272668096,\n",
       "   0.31190125391728857,\n",
       "   0.2983298584868346,\n",
       "   0.29847728998162026,\n",
       "   0.30677279602253554,\n",
       "   0.29605747764292745,\n",
       "   0.29675809222202365,\n",
       "   0.2941265337391945,\n",
       "   0.30432964336634477,\n",
       "   0.2978988795779472,\n",
       "   0.3056095379135537,\n",
       "   0.29778451479946655,\n",
       "   0.29742139973909754,\n",
       "   0.29716582853730733,\n",
       "   0.2984103760449989,\n",
       "   0.2980535753145566,\n",
       "   0.3041933555143616,\n",
       "   0.29587071688468275,\n",
       "   0.29064537084775904,\n",
       "   0.29866083624751066,\n",
       "   0.29299706359242284,\n",
       "   0.29794324097839303,\n",
       "   0.30191464152843056,\n",
       "   0.29728986644665667,\n",
       "   0.2994510769051967,\n",
       "   0.2961885801878482,\n",
       "   0.29417409354270097,\n",
       "   0.30369055667192835,\n",
       "   0.29874650020139953,\n",
       "   0.2962846276768022,\n",
       "   0.3014107164354419,\n",
       "   0.3012777797605508,\n",
       "   0.29719483050397066,\n",
       "   0.3049909437811652,\n",
       "   0.3027915468445648,\n",
       "   0.2964656821319035,\n",
       "   0.3012861493616009,\n",
       "   0.29618265749608164,\n",
       "   0.2962740764665445,\n",
       "   0.28882259239390046,\n",
       "   0.294476929792138],\n",
       "  'val_loss': [0.4666139278852828,\n",
       "   0.33849874774116706,\n",
       "   0.28369252972281467,\n",
       "   0.278026612294505,\n",
       "   0.24955495622090784,\n",
       "   0.2527802425865843,\n",
       "   0.25127032305753344,\n",
       "   0.2458479278326782,\n",
       "   0.24069572578776965,\n",
       "   0.24528278013381838,\n",
       "   0.2383729535210469,\n",
       "   0.24319839767154108,\n",
       "   0.24037579337257575,\n",
       "   0.24059213567116416,\n",
       "   0.25082031032508445,\n",
       "   0.2380029812315041,\n",
       "   0.2358061857171193,\n",
       "   0.23984272338940432,\n",
       "   0.23957239156793278,\n",
       "   0.25021033918595986,\n",
       "   0.23477337633180767,\n",
       "   0.2322537920972023,\n",
       "   0.2331489714615771,\n",
       "   0.23182803462478435,\n",
       "   0.23625212258492892,\n",
       "   0.23737446104284365,\n",
       "   0.24027684214159986,\n",
       "   0.24097222299971924,\n",
       "   0.24023144208712263,\n",
       "   0.23606579650345266,\n",
       "   0.23762777242167243,\n",
       "   0.23884253082417395,\n",
       "   0.2285673970525915,\n",
       "   0.2257860431383396,\n",
       "   0.2256534566699898,\n",
       "   0.23746710585949937,\n",
       "   0.24100939518418804,\n",
       "   0.2357050057582347,\n",
       "   0.2418806622955119,\n",
       "   0.24260225172700553,\n",
       "   0.22864675138811333,\n",
       "   0.23613248494538394,\n",
       "   0.2254271638524196,\n",
       "   0.2366428495294248,\n",
       "   0.22983679119322367,\n",
       "   0.23452235716263703,\n",
       "   0.23292566686401547,\n",
       "   0.23136348153543324,\n",
       "   0.23118887439977412,\n",
       "   0.22996489189822097,\n",
       "   0.22943311065528835,\n",
       "   0.2350739904892482,\n",
       "   0.2416599944560879,\n",
       "   0.2317476262773466,\n",
       "   0.23437516199757685,\n",
       "   0.22989698011299659,\n",
       "   0.229493931075996,\n",
       "   0.23079407803690918,\n",
       "   0.24131550268506555,\n",
       "   0.24110100004927118,\n",
       "   0.22859326131104676,\n",
       "   0.2311774756654303,\n",
       "   0.24070807189029586,\n",
       "   0.23336708209357665,\n",
       "   0.23665369435163874,\n",
       "   0.23110910334557203,\n",
       "   0.2295476540596134,\n",
       "   0.24001087570638865,\n",
       "   0.2311361356401892,\n",
       "   0.23631632500868233,\n",
       "   0.22939609775909436,\n",
       "   0.23798457101034146,\n",
       "   0.23003720022667912,\n",
       "   0.23356123310645172,\n",
       "   0.2266016620733895,\n",
       "   0.2272405368407318,\n",
       "   0.2339196179447503,\n",
       "   0.23248240505826884,\n",
       "   0.22903495517830863,\n",
       "   0.2372286621492858,\n",
       "   0.22492745518684387,\n",
       "   0.2314776958062731,\n",
       "   0.23312777892736058,\n",
       "   0.2266231713156715,\n",
       "   0.23192699753378626,\n",
       "   0.23598645277157845,\n",
       "   0.23712035382989805,\n",
       "   0.2278169834501691,\n",
       "   0.22677157543669674,\n",
       "   0.22944937590334483,\n",
       "   0.23781087485413566,\n",
       "   0.23030394138213608,\n",
       "   0.2288963726975701,\n",
       "   0.23445406458034038,\n",
       "   0.22609048528263936,\n",
       "   0.23214305390758574,\n",
       "   0.24510155487396873,\n",
       "   0.2365676543462239,\n",
       "   0.23938103730215174,\n",
       "   0.23431617114984876,\n",
       "   0.22636076041897263,\n",
       "   0.2282738937386151,\n",
       "   0.22599534022397008,\n",
       "   0.23091104224931483,\n",
       "   0.23284187258971523,\n",
       "   0.2338755796415305,\n",
       "   0.22774479003348694,\n",
       "   0.22786449830173325,\n",
       "   0.22669166518042455,\n",
       "   0.22777429923742168,\n",
       "   0.23325969129333676,\n",
       "   0.22982478982602542,\n",
       "   0.2328045079046656,\n",
       "   0.23475908079304292,\n",
       "   0.2370137501286115,\n",
       "   0.23113014084231517,\n",
       "   0.2302409658517957,\n",
       "   0.22943988129449863,\n",
       "   0.22781678455003002,\n",
       "   0.23329037493299168,\n",
       "   0.2270309445813158,\n",
       "   0.23026156635680542,\n",
       "   0.22744990254644315,\n",
       "   0.2249178595864287,\n",
       "   0.2300438304960167,\n",
       "   0.22860075346541628,\n",
       "   0.230725623597172,\n",
       "   0.23446183616651636,\n",
       "   0.22530016116214022,\n",
       "   0.22682345455343073,\n",
       "   0.2197580064959287,\n",
       "   0.22226208210178303,\n",
       "   0.23496079608377618,\n",
       "   0.22932387729610396,\n",
       "   0.22913471013774692,\n",
       "   0.22828961053985786,\n",
       "   0.23047423105815362,\n",
       "   0.23330113343123732,\n",
       "   0.22712819301596257,\n",
       "   0.2291704447572134,\n",
       "   0.23313075607660047,\n",
       "   0.2298589779011508,\n",
       "   0.2275903809780611,\n",
       "   0.22839781894205505,\n",
       "   0.22666465035613428,\n",
       "   0.22410192819411479,\n",
       "   0.2341446045517548,\n",
       "   0.22780670150880902,\n",
       "   0.22986424735347305,\n",
       "   0.2213898514832449,\n",
       "   0.23937849340767697,\n",
       "   0.22626225285956106,\n",
       "   0.22132813176110994,\n",
       "   0.2254163425731061,\n",
       "   0.22584946383503163,\n",
       "   0.22629475701004734,\n",
       "   0.22661892625979121,\n",
       "   0.22307163803928698,\n",
       "   0.22585884684866125,\n",
       "   0.23079192470233642,\n",
       "   0.22739995390083348,\n",
       "   0.23136362741733418,\n",
       "   0.22897029198636082,\n",
       "   0.23025581619022034,\n",
       "   0.23799634773903133,\n",
       "   0.22682541823686103,\n",
       "   0.23397157408974387,\n",
       "   0.22298850511495596,\n",
       "   0.22193454098551998,\n",
       "   0.2372264668672436,\n",
       "   0.23313729745280407,\n",
       "   0.22686945242941567,\n",
       "   0.23142686598353251,\n",
       "   0.22834140529453195,\n",
       "   0.22040644213323682,\n",
       "   0.21965935471289583,\n",
       "   0.22362515363013108,\n",
       "   0.2293037843181048,\n",
       "   0.21870141100360307,\n",
       "   0.22858005355704913,\n",
       "   0.2200671299871606,\n",
       "   0.22973368823715137,\n",
       "   0.22528240009908765,\n",
       "   0.22610238624216994,\n",
       "   0.22586100242541501,\n",
       "   0.22425344002471076,\n",
       "   0.22918417657430643,\n",
       "   0.22851334446836788,\n",
       "   0.22853140747846107,\n",
       "   0.22723182475305276,\n",
       "   0.22574932387443172,\n",
       "   0.22389814519209547,\n",
       "   0.22690016577796876,\n",
       "   0.22409903620104057,\n",
       "   0.22272626621222422,\n",
       "   0.22104947266627256,\n",
       "   0.22939181514668242,\n",
       "   0.22575957059486532,\n",
       "   0.22781082333815883,\n",
       "   0.22331112300901204,\n",
       "   0.2309551448751019,\n",
       "   0.22340571396870298,\n",
       "   0.2375858895550701,\n",
       "   0.22309414874236785,\n",
       "   0.225033252450366,\n",
       "   0.22629020365428026,\n",
       "   0.2344740857151235,\n",
       "   0.22275638711116158,\n",
       "   0.2303707982679146,\n",
       "   0.2233148263913337,\n",
       "   0.22742543385880867,\n",
       "   0.23021052446111243,\n",
       "   0.23058511611062532,\n",
       "   0.2310732289167781,\n",
       "   0.2236450042657344,\n",
       "   0.22850766352912102,\n",
       "   0.2253748669192709,\n",
       "   0.22324588549174484,\n",
       "   0.23050987734503134,\n",
       "   0.23192305792835438,\n",
       "   0.22552552800567172,\n",
       "   0.22745994999304087,\n",
       "   0.22868526444352907,\n",
       "   0.2252923248797001,\n",
       "   0.22539118086469584,\n",
       "   0.22728435704513777,\n",
       "   0.2242463253508541,\n",
       "   0.2305652572069796,\n",
       "   0.2292026345352394,\n",
       "   0.233733475441843,\n",
       "   0.23513387330460325,\n",
       "   0.22651186291140074,\n",
       "   0.2252017761752897,\n",
       "   0.2238647383301983,\n",
       "   0.2294928993457537,\n",
       "   0.22237550882990456,\n",
       "   0.22561138835633437,\n",
       "   0.23065433994542842,\n",
       "   0.22650935208909564,\n",
       "   0.22645128385213475,\n",
       "   0.22675932173071237,\n",
       "   0.22295237479725602,\n",
       "   0.223510554544978,\n",
       "   0.22019910555461358,\n",
       "   0.2184304189999649,\n",
       "   0.22436568762254566,\n",
       "   0.22482403276669194,\n",
       "   0.2295259413861182,\n",
       "   0.22068505456268228,\n",
       "   0.2304033049875666,\n",
       "   0.2274872559738757,\n",
       "   0.22025388250343478,\n",
       "   0.2208404984079932,\n",
       "   0.22679477210702567,\n",
       "   0.2286520374699446,\n",
       "   0.22963308704123603,\n",
       "   0.23092929701259518,\n",
       "   0.22557525803863443,\n",
       "   0.23013214752965586,\n",
       "   0.22260286883220404,\n",
       "   0.2295993665542722,\n",
       "   0.22732440557412592,\n",
       "   0.2274890237000295,\n",
       "   0.22343282247411794,\n",
       "   0.22427980741923878,\n",
       "   0.221481949698215,\n",
       "   0.22599511077411497,\n",
       "   0.23292388960859245,\n",
       "   0.21930156823329416,\n",
       "   0.22269443289426427,\n",
       "   0.22171358698774654,\n",
       "   0.2310200657777278,\n",
       "   0.22459911753578246,\n",
       "   0.22416193703872656,\n",
       "   0.22421221855478973,\n",
       "   0.22855223033308608,\n",
       "   0.21936257223257077,\n",
       "   0.2267069155145962,\n",
       "   0.2222315884402553,\n",
       "   0.22976845330018608,\n",
       "   0.2222689825539305,\n",
       "   0.2295037862088613,\n",
       "   0.2190905949071657,\n",
       "   0.23130313449518808,\n",
       "   0.22553981562673486,\n",
       "   0.2330215133749953,\n",
       "   0.22270245196116756,\n",
       "   0.23717274584553458,\n",
       "   0.22648118310213836,\n",
       "   0.22823965320579684,\n",
       "   0.23014618928155928,\n",
       "   0.2257092736170957,\n",
       "   0.2271111760300155,\n",
       "   0.22458305371031867,\n",
       "   0.2314251047970733,\n",
       "   0.22690729834256126,\n",
       "   0.22660933420949594,\n",
       "   0.22196521939341926,\n",
       "   0.2211426043398328,\n",
       "   0.22488637408679554],\n",
       "  'train_acc': [61.84939091915836,\n",
       "   76.10741971207088,\n",
       "   83.69324473975637,\n",
       "   85.71428571428571,\n",
       "   86.96013289036544,\n",
       "   87.59689922480621,\n",
       "   88.56589147286822,\n",
       "   88.62126245847176,\n",
       "   89.06423034330011,\n",
       "   88.92580287929125,\n",
       "   88.7873754152824,\n",
       "   89.7563676633444,\n",
       "   89.53488372093024,\n",
       "   89.97785160575859,\n",
       "   89.23034330011075,\n",
       "   89.72868217054264,\n",
       "   89.50719822812846,\n",
       "   90.00553709856035,\n",
       "   89.7563676633444,\n",
       "   89.64562569213732,\n",
       "   89.562569213732,\n",
       "   90.22702104097453,\n",
       "   89.64562569213732,\n",
       "   90.55924695459579,\n",
       "   89.78405315614619,\n",
       "   90.11627906976744,\n",
       "   90.03322259136213,\n",
       "   89.97785160575859,\n",
       "   89.42414174972315,\n",
       "   89.95016611295681,\n",
       "   90.0609080841639,\n",
       "   89.89479512735326,\n",
       "   90.17165005537099,\n",
       "   89.89479512735326,\n",
       "   90.58693244739756,\n",
       "   89.92248062015504,\n",
       "   90.08859357696566,\n",
       "   90.00553709856035,\n",
       "   89.97785160575859,\n",
       "   90.2547065337763,\n",
       "   90.17165005537099,\n",
       "   90.28239202657808,\n",
       "   89.8671096345515,\n",
       "   90.47619047619048,\n",
       "   90.03322259136213,\n",
       "   90.0609080841639,\n",
       "   90.2547065337763,\n",
       "   90.47619047619048,\n",
       "   90.00553709856035,\n",
       "   90.22702104097453,\n",
       "   90.00553709856035,\n",
       "   89.81173864894795,\n",
       "   90.53156146179403,\n",
       "   90.36544850498339,\n",
       "   90.28239202657808,\n",
       "   90.14396456256921,\n",
       "   89.89479512735326,\n",
       "   90.53156146179403,\n",
       "   89.92248062015504,\n",
       "   90.31007751937985,\n",
       "   90.69767441860465,\n",
       "   90.58693244739756,\n",
       "   90.17165005537099,\n",
       "   90.53156146179403,\n",
       "   90.14396456256921,\n",
       "   90.03322259136213,\n",
       "   90.28239202657808,\n",
       "   90.47619047619048,\n",
       "   90.6423034330011,\n",
       "   90.39313399778516,\n",
       "   89.95016611295681,\n",
       "   90.14396456256921,\n",
       "   90.33776301218161,\n",
       "   90.31007751937985,\n",
       "   90.4485049833887,\n",
       "   90.42081949058694,\n",
       "   90.42081949058694,\n",
       "   90.36544850498339,\n",
       "   90.14396456256921,\n",
       "   90.33776301218161,\n",
       "   89.95016611295681,\n",
       "   90.00553709856035,\n",
       "   89.95016611295681,\n",
       "   90.6423034330011,\n",
       "   90.17165005537099,\n",
       "   90.31007751937985,\n",
       "   90.39313399778516,\n",
       "   90.50387596899225,\n",
       "   90.33776301218161,\n",
       "   90.53156146179403,\n",
       "   90.39313399778516,\n",
       "   90.31007751937985,\n",
       "   90.22702104097453,\n",
       "   90.55924695459579,\n",
       "   90.39313399778516,\n",
       "   90.66998892580288,\n",
       "   90.31007751937985,\n",
       "   90.61461794019934,\n",
       "   90.53156146179403,\n",
       "   90.53156146179403,\n",
       "   90.0609080841639,\n",
       "   90.50387596899225,\n",
       "   90.28239202657808,\n",
       "   90.03322259136213,\n",
       "   90.42081949058694,\n",
       "   90.11627906976744,\n",
       "   90.17165005537099,\n",
       "   90.72535991140643,\n",
       "   90.53156146179403,\n",
       "   90.50387596899225,\n",
       "   90.31007751937985,\n",
       "   90.11627906976744,\n",
       "   90.53156146179403,\n",
       "   90.78073089700996,\n",
       "   90.47619047619048,\n",
       "   90.39313399778516,\n",
       "   90.66998892580288,\n",
       "   90.08859357696566,\n",
       "   90.2547065337763,\n",
       "   89.92248062015504,\n",
       "   90.53156146179403,\n",
       "   90.55924695459579,\n",
       "   90.36544850498339,\n",
       "   90.6423034330011,\n",
       "   90.33776301218161,\n",
       "   89.97785160575859,\n",
       "   90.17165005537099,\n",
       "   90.4485049833887,\n",
       "   90.6423034330011,\n",
       "   90.39313399778516,\n",
       "   90.36544850498339,\n",
       "   90.58693244739756,\n",
       "   90.55924695459579,\n",
       "   90.31007751937985,\n",
       "   90.17165005537099,\n",
       "   90.55924695459579,\n",
       "   90.28239202657808,\n",
       "   89.95016611295681,\n",
       "   90.36544850498339,\n",
       "   90.8361018826135,\n",
       "   89.95016611295681,\n",
       "   90.0609080841639,\n",
       "   90.33776301218161,\n",
       "   90.61461794019934,\n",
       "   90.31007751937985,\n",
       "   90.69767441860465,\n",
       "   90.47619047619048,\n",
       "   90.69767441860465,\n",
       "   89.7563676633444,\n",
       "   90.69767441860465,\n",
       "   90.66998892580288,\n",
       "   90.47619047619048,\n",
       "   90.28239202657808,\n",
       "   90.61461794019934,\n",
       "   90.47619047619048,\n",
       "   89.89479512735326,\n",
       "   90.4485049833887,\n",
       "   90.50387596899225,\n",
       "   90.47619047619048,\n",
       "   90.39313399778516,\n",
       "   90.19933554817275,\n",
       "   90.72535991140643,\n",
       "   90.7530454042082,\n",
       "   90.2547065337763,\n",
       "   90.66998892580288,\n",
       "   90.14396456256921,\n",
       "   90.47619047619048,\n",
       "   90.6423034330011,\n",
       "   90.50387596899225,\n",
       "   90.19933554817275,\n",
       "   90.11627906976744,\n",
       "   90.33776301218161,\n",
       "   90.61461794019934,\n",
       "   90.72535991140643,\n",
       "   90.86378737541528,\n",
       "   90.47619047619048,\n",
       "   90.33776301218161,\n",
       "   90.58693244739756,\n",
       "   90.6423034330011,\n",
       "   90.28239202657808,\n",
       "   90.61461794019934,\n",
       "   90.08859357696566,\n",
       "   90.0609080841639,\n",
       "   90.80841638981174,\n",
       "   90.28239202657808,\n",
       "   90.33776301218161,\n",
       "   90.39313399778516,\n",
       "   90.53156146179403,\n",
       "   90.78073089700996,\n",
       "   90.36544850498339,\n",
       "   90.47619047619048,\n",
       "   90.47619047619048,\n",
       "   90.42081949058694,\n",
       "   90.42081949058694,\n",
       "   90.31007751937985,\n",
       "   90.4485049833887,\n",
       "   90.31007751937985,\n",
       "   90.36544850498339,\n",
       "   90.42081949058694,\n",
       "   90.39313399778516,\n",
       "   90.28239202657808,\n",
       "   90.47619047619048,\n",
       "   90.31007751937985,\n",
       "   90.39313399778516,\n",
       "   90.39313399778516,\n",
       "   90.80841638981174,\n",
       "   90.72535991140643,\n",
       "   90.53156146179403,\n",
       "   90.72535991140643,\n",
       "   90.39313399778516,\n",
       "   90.8361018826135,\n",
       "   90.33776301218161,\n",
       "   90.36544850498339,\n",
       "   90.50387596899225,\n",
       "   90.33776301218161,\n",
       "   90.2547065337763,\n",
       "   90.42081949058694,\n",
       "   90.39313399778516,\n",
       "   90.47619047619048,\n",
       "   90.66998892580288,\n",
       "   90.39313399778516,\n",
       "   90.39313399778516,\n",
       "   90.61461794019934,\n",
       "   90.86378737541528,\n",
       "   90.69767441860465,\n",
       "   90.55924695459579,\n",
       "   90.31007751937985,\n",
       "   90.22702104097453,\n",
       "   90.4485049833887,\n",
       "   91.05758582502769,\n",
       "   90.55924695459579,\n",
       "   90.28239202657808,\n",
       "   90.22702104097453,\n",
       "   90.50387596899225,\n",
       "   90.8361018826135,\n",
       "   90.39313399778516,\n",
       "   90.53156146179403,\n",
       "   90.36544850498339,\n",
       "   90.47619047619048,\n",
       "   90.66998892580288,\n",
       "   90.39313399778516,\n",
       "   90.53156146179403,\n",
       "   90.28239202657808,\n",
       "   90.31007751937985,\n",
       "   90.4485049833887,\n",
       "   90.61461794019934,\n",
       "   90.89147286821705,\n",
       "   90.03322259136213,\n",
       "   90.55924695459579,\n",
       "   90.50387596899225,\n",
       "   90.7530454042082,\n",
       "   90.33776301218161,\n",
       "   90.55924695459579,\n",
       "   90.72535991140643,\n",
       "   90.08859357696566,\n",
       "   90.69767441860465,\n",
       "   90.8361018826135,\n",
       "   90.4485049833887,\n",
       "   90.58693244739756,\n",
       "   90.72535991140643,\n",
       "   89.83942414174972,\n",
       "   90.28239202657808,\n",
       "   90.50387596899225,\n",
       "   90.28239202657808,\n",
       "   90.42081949058694,\n",
       "   90.55924695459579,\n",
       "   90.53156146179403,\n",
       "   90.61461794019934,\n",
       "   90.6423034330011,\n",
       "   90.17165005537099,\n",
       "   90.33776301218161,\n",
       "   90.55924695459579,\n",
       "   90.69767441860465,\n",
       "   90.58693244739756,\n",
       "   90.55924695459579,\n",
       "   90.33776301218161,\n",
       "   90.58693244739756,\n",
       "   90.6423034330011,\n",
       "   90.53156146179403,\n",
       "   90.78073089700996,\n",
       "   90.53156146179403,\n",
       "   90.47619047619048,\n",
       "   90.78073089700996,\n",
       "   90.39313399778516,\n",
       "   90.55924695459579,\n",
       "   90.86378737541528,\n",
       "   90.53156146179403,\n",
       "   90.31007751937985,\n",
       "   90.69767441860465,\n",
       "   90.42081949058694,\n",
       "   90.8361018826135,\n",
       "   90.39313399778516,\n",
       "   90.61461794019934,\n",
       "   90.39313399778516,\n",
       "   90.53156146179403,\n",
       "   90.4485049833887,\n",
       "   90.72535991140643,\n",
       "   90.61461794019934,\n",
       "   90.78073089700996,\n",
       "   90.47619047619048],\n",
       "  'val_acc': [85.26645768025078,\n",
       "   89.65517241379311,\n",
       "   90.9090909090909,\n",
       "   90.75235109717869,\n",
       "   92.6332288401254,\n",
       "   92.47648902821317,\n",
       "   92.47648902821317,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   92.47648902821317,\n",
       "   93.41692789968653,\n",
       "   92.6332288401254,\n",
       "   92.6332288401254,\n",
       "   93.10344827586206,\n",
       "   92.00626959247649,\n",
       "   92.78996865203762,\n",
       "   92.78996865203762,\n",
       "   93.2601880877743,\n",
       "   92.6332288401254,\n",
       "   92.00626959247649,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   92.78996865203762,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   91.84952978056427,\n",
       "   92.94670846394985,\n",
       "   93.41692789968653,\n",
       "   92.16300940438872,\n",
       "   92.31974921630093,\n",
       "   93.10344827586206,\n",
       "   94.04388714733543,\n",
       "   93.73040752351098,\n",
       "   93.73040752351098,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   92.94670846394985,\n",
       "   92.6332288401254,\n",
       "   93.57366771159874,\n",
       "   93.10344827586206,\n",
       "   92.94670846394985,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   92.94670846394985,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   92.00626959247649,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   92.78996865203762,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   92.94670846394985,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   92.94670846394985,\n",
       "   92.94670846394985,\n",
       "   92.78996865203762,\n",
       "   92.78996865203762,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   92.78996865203762,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   92.78996865203762,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   92.78996865203762,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   92.94670846394985,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   92.94670846394985,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   92.6332288401254,\n",
       "   92.94670846394985,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.8871473354232,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   92.78996865203762,\n",
       "   93.57366771159874,\n",
       "   93.73040752351098,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   94.04388714733543,\n",
       "   93.41692789968653,\n",
       "   93.73040752351098,\n",
       "   92.94670846394985,\n",
       "   93.57366771159874,\n",
       "   93.10344827586206,\n",
       "   93.8871473354232,\n",
       "   93.8871473354232,\n",
       "   92.94670846394985,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   92.78996865203762,\n",
       "   92.94670846394985,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.57366771159874,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   93.57366771159874,\n",
       "   92.94670846394985,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.41692789968653,\n",
       "   93.73040752351098,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.57366771159874,\n",
       "   94.04388714733543,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   93.8871473354232,\n",
       "   93.2601880877743,\n",
       "   93.73040752351098,\n",
       "   93.73040752351098,\n",
       "   93.8871473354232,\n",
       "   94.04388714733543,\n",
       "   93.41692789968653,\n",
       "   93.57366771159874,\n",
       "   94.04388714733543,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   93.10344827586206,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.73040752351098,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.73040752351098,\n",
       "   93.8871473354232,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.73040752351098,\n",
       "   94.04388714733543,\n",
       "   94.35736677115987,\n",
       "   94.04388714733543,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.8871473354232,\n",
       "   93.41692789968653,\n",
       "   94.04388714733543,\n",
       "   93.2601880877743,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   92.6332288401254,\n",
       "   93.2601880877743,\n",
       "   93.8871473354232,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   92.78996865203762,\n",
       "   93.57366771159874,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   93.57366771159874,\n",
       "   93.57366771159874,\n",
       "   94.20062695924764,\n",
       "   93.8871473354232,\n",
       "   94.04388714733543,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   93.10344827586206,\n",
       "   93.10344827586206,\n",
       "   93.57366771159874,\n",
       "   93.8871473354232,\n",
       "   93.41692789968653,\n",
       "   93.2601880877743,\n",
       "   93.73040752351098,\n",
       "   93.41692789968653,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   94.04388714733543,\n",
       "   93.73040752351098,\n",
       "   93.8871473354232,\n",
       "   94.04388714733543,\n",
       "   93.8871473354232,\n",
       "   93.8871473354232,\n",
       "   93.57366771159874,\n",
       "   93.73040752351098,\n",
       "   94.20062695924764,\n",
       "   93.73040752351098,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   92.6332288401254,\n",
       "   93.57366771159874,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.10344827586206,\n",
       "   94.04388714733543,\n",
       "   93.57366771159874,\n",
       "   93.57366771159874,\n",
       "   93.8871473354232,\n",
       "   92.6332288401254,\n",
       "   93.2601880877743,\n",
       "   93.8871473354232,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.8871473354232,\n",
       "   93.8871473354232,\n",
       "   93.2601880877743,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   93.57366771159874,\n",
       "   94.04388714733543,\n",
       "   93.8871473354232,\n",
       "   94.04388714733543,\n",
       "   94.04388714733543,\n",
       "   93.41692789968653,\n",
       "   93.73040752351098,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.57366771159874,\n",
       "   93.2601880877743,\n",
       "   93.2601880877743,\n",
       "   93.10344827586206,\n",
       "   93.73040752351098,\n",
       "   93.41692789968653,\n",
       "   93.8871473354232,\n",
       "   93.2601880877743,\n",
       "   93.41692789968653,\n",
       "   93.41692789968653,\n",
       "   93.73040752351098,\n",
       "   93.57366771159874,\n",
       "   93.73040752351098]},\n",
       " 'best_train_metrics': {'loss': 0.2994243035284784,\n",
       "  'accuracy': 90.4485049833887,\n",
       "  'precision': 0.8819538670284939,\n",
       "  'recall': 0.8837525492861998,\n",
       "  'f1': 0.8828522920203735,\n",
       "  'auroc': 0.9255251219990023},\n",
       " 'best_val_metrics': {'loss': 0.2184304189999649,\n",
       "  'accuracy': 93.73040752351098,\n",
       "  'precision': 0.9230769230769231,\n",
       "  'recall': 0.9230769230769231,\n",
       "  'f1': 0.9230769230769231,\n",
       "  'auroc': 0.9513329263329264},\n",
       " 'best_epoch': 245,\n",
       " 'total_epochs': 300,\n",
       " 'model': Sequential(\n",
       "   (0): Linear(in_features=11, out_features=32, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (3): Dropout(p=0.6, inplace=False)\n",
       "   (4): Linear(in_features=32, out_features=16, bias=True)\n",
       "   (5): ReLU()\n",
       "   (6): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (7): Dropout(p=0.5086790371135944, inplace=False)\n",
       "   (8): Linear(in_features=16, out_features=16, bias=True)\n",
       "   (9): ReLU()\n",
       "   (10): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "   (11): Dropout(p=0.1293341508821494, inplace=False)\n",
       "   (12): Linear(in_features=16, out_features=2, bias=True)\n",
       " )}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6e0e00a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RESULTADOS DEL CONJUNTO DE PRUEBA (usando el mejor modelo):\n",
      "Prdida: 0.3034, Accuracy: 90.53%, Precision: 0.8730, Recall: 0.8987, F1: 0.8857, AUROC: 0.9163\n"
     ]
    }
   ],
   "source": [
    "# Load best model and evaluate on test set\n",
    "mlp.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss, test_acc, test_labels, test_preds, test_probs = eval_epoch(mlp, test_loader, criterion, device)\n",
    "test_precision, test_recall, test_f1, test_auroc = _compute_metrics(test_labels, test_preds, test_probs)\n",
    "\n",
    "print(f\"\\nRESULTADOS DEL CONJUNTO DE PRUEBA (usando el mejor modelo):\")\n",
    "print(f\"Prdida: {test_loss:.4f}, Accuracy: {test_acc:.2f}%, Precision: {test_precision:.4f}, \"\n",
    "      f\"Recall: {test_recall:.4f}, F1: {test_f1:.4f}, AUROC: {test_auroc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6529343e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "Sequential                               [1, 2]                    --\n",
       "Linear: 1-1                            [1, 32]                   384\n",
       "ReLU: 1-2                              [1, 32]                   --\n",
       "BatchNorm1d: 1-3                       [1, 32]                   64\n",
       "Dropout: 1-4                           [1, 32]                   --\n",
       "Linear: 1-5                            [1, 16]                   528\n",
       "ReLU: 1-6                              [1, 16]                   --\n",
       "BatchNorm1d: 1-7                       [1, 16]                   32\n",
       "Dropout: 1-8                           [1, 16]                   --\n",
       "Linear: 1-9                            [1, 16]                   272\n",
       "ReLU: 1-10                             [1, 16]                   --\n",
       "BatchNorm1d: 1-11                      [1, 16]                   32\n",
       "Dropout: 1-12                          [1, 16]                   --\n",
       "Linear: 1-13                           [1, 2]                    34\n",
       "==========================================================================================\n",
       "Total params: 1,346\n",
       "Trainable params: 1,346\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.00\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.01\n",
       "Estimated Total Size (MB): 0.01\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(mlp, input_size=(1, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35779360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-si",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
